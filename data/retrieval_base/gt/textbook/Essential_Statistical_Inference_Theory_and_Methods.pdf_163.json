[
    {
        "text": "$\n\\caption{Table 3.3 $P$ -values for three samples. $n_1=n_2=n_3=5$, $\\sigma^2=14$}\n\\begin{tabular}{|c|c|c|c|c|c|}\n \n$\\bar{Y}_1$ & $\\bar{Y}_2$ & $\\bar{Y}_3$ & ANOVA & ISO & Reg \\\\\n \n2 & 6 & 6 & 0.149 & 0.050 & 0.045 \\\\\n \n2 & 6 & 7 & 0.082 & 0.026 & 0.017 \\\\\n \n2 & 6 & 8 & 0.036 & 0.011 & 0.006 \\\\\n \n\\end{tabular}\n$\n\nrelative ease of using regression is probably why isotonic regression is not used more.  \n\nFinally, although the $p$ -values reported in Table 3.3 are for the known-variance case, qualitatively similar results are obtained for the more complicated case of $\\sigma$ unknown.\n\nThere is a large literature on order-restricted inference. For testing for ordered alternatives, there has been more emphasis on $T_{\\mathrm{LR}}$ than on $T_{\\mathrm{{W}}}$ and $T_{S}$. The classic references are Barlow et al. (1972) and Robertson et al. (1988), whereas a more recent account is Silva pull e and Sen (2005).  \n\n# 3.6.2 Null Hypotheses on the Boundary of the Parameter Space  \n\nWhen a null hypothesis value, say $\\pmb{\\theta}_{0}$ lies on the boundary of the parameter space, then maximum likelihood estimators are often truncated at that boundary because by definition $\\widehat{\\pmb{\\theta}}_{\\mathrm{MLE}}$ must lie in the parameter space of $\\theta$. Thus, $\\widehat{\\pmb{\\theta}}_{\\mathrm{MLE}}$ is equal to the boundary value $\\pmb{\\theta}_{0}$ with positive probability and correspondingly $T_{\\mathrm{LR}}$ is zero for those cases. The result is that the limiting distribution of $T_{\\mathrm{LR}}$ is a mixture of a point mass at zero and a chi-squared distribution. We illustrate first with an artificial example and then consider the one-way random effects model.  \n\n# 3.6.2 a Normal Mean with Restricted Parameter Space  \n\nSuppose that $Y_{1}, .\\,.\\,.\\,, Y_{n} \\sim N(\\mu, 1)$. Usually, $\\widehat{\\mu}_{\\mathrm{MLE}} = \\overline{{Y}}$, but suppose that we restrict the parameter space for $\\mu$ to be $[\\mu_{0}, \\infty)$ where $\\mu_{0}$ is some given constant, instead of $(-\\infty, \\infty)$. Then $\\widehat{\\mu}_{\\mathrm{MLE}} = \\overline{{Y}}$ if $\\overline{{Y}} \\geq \\mu_{0}$ and $\\widehat{\\mu}_{\\mathrm{MLE}} = \\mu_{0}$ if $\\overline{{Y}} < \\mu_{0}$. Now suppose that the null hypothesis is $H_{0}: \\mu = \\mu_{0}$. We first consider the three likelihood-based test statistics, showing that only the score statistic has a limiting $\\chi_{1}^{2}$ distribution. Then we provide a simple solution to this testing problem.  \n\nUnder $H_{0}$, the Wald statistic is $T_{\\mathrm{W}} = n(\\widehat{\\mu}_{\\mathrm{MLE}} - \\mu_{0})^{2}$, which is thus $T_{\\mathrm{W}} = 0$ if $\\widehat{\\mu}_{\\mathrm{MLE}} = \\mu_{0}$ and $T_{\\mathrm{W}} = n(\\overline{{Y}} - \\mu_{0})^{2}$ if $Y \\geq \\mu_{0}$. The score statistic is $T_{\\mathrm{S}} = n(Y - \\mu_{0})^{2}$ and the likelihood ratio statistic is the same as the Wald statistic. Thus, only the score statistic converges to a $\\chi_{1}^{2}$ distribution under $H_{0}$. The Wald and the likelihood ratio statistics converge to a distribution that is an equal mixture of a point mass at 0 and a $\\chi_{1}^{2}$ distribution, the same distribution as in (3.23) for $k = 2$. In fact, the",
        "page_idx": 0
    }
]