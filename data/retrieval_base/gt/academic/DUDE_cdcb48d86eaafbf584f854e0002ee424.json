[
    {
        "text": "Many voices with many questions\n\nIntroduction\n\nThe Wikimedia movement has a history of doing surveys of their communities．［1］While the Wikimedia Foundation has traditionally conducted major editor surveys，it has been a few years since the last one was completed in 2012．Since then，demand for survey data has increased within the Foundation．Teams need data to make decisions about their work that affects communities．Teams often support the same communities，but they have very different goals．While the Survey Support Desk［2］is meant to support teams with their surveys，it would be challenging for one person to support them all．Therefore，doing surveys in this context brings up a two key problems：\n1．Complex surveys take time and expertise Doing a complex survey can be too much work for a small group of people，and teams at the Foundation can be quite small．\nSurveys require significant expertise related to communication，planning，translation， analysis and reporting．Teams have to have the skills required，which often are not located in any one team．\n2．Surveying the same people can cause fatigue．With teams sharing the communities they serve，a major risk was surveying communities too much，and causing survey fatigue for all who do surveys．\n\nWith this in mind，how could we design a process that can include all these voices who need to ask questions to the same communities？\n\nCitations\n［1］meta：Category：Surveys\n［2］meta：Surveys\n［3］meta：Community＿Engagement＿Insights\n［4］meta：Community＿Engagement＿Insights／Sampling＿strategy ［5］meta：CEInsights／2016－17＿Survey＿report\n\nDesigning the process\n\nFirst，understanding the level of demand for surveys was crucial．In early Spring 2016，interviews were conducted with Wikimedia Foundation staff across various departments．We asked two questions：＂Who are the communities you serve？＂；and＂what long－term data do you need to make a decision？＂\nThe main audiences identified were：Editors， Affiliates，Program leaders Volunteer developers， External partners，External researchers．\nThe main data needs were attitudes，attributes \\＆ behaviors about：demographics，Wikimedia projects， software，programs，affiliates，and Wikimedia Foundation products \\＆ programs.\n\nOverall，demand for survey data was very high，and a process was needed that would encourage only those teams that needed data the most．We chose three ways to do this：\n－A proposal－based process：Create a proposal process for teams to decide on their own if they can participate，with a heavy focus on team or project goals for choosing questions．\n－Limit number of questions to 20 per team．\n－Donation requirement：Teams were asked to donate time to the project：each team was expected to participate in one of 5 working groups：Communications，Community， Translations，Survey design，and Analysis\n\nResults\n\nIn mid－2016，the survey process was implemented and called Community Engagement Insights．［3］ The goal of the project was to（1）use surveys in order to improve alignment between the Foundation and Wikimedia communities（2）gather community feedback \\＆needs（3）use it as a way to prevent survey fatigue．The survey went out to very active editors，active editors，Wikimedia affiliates，program leaders，and volunteer developers．\n\nWhat went well？\n－Preparing the questions： 13 teams contributed questions from the Foundation．\n－Administering the survey：At least 15 staff members supported the project in various ways．\n－Volunteer support：Many community members supported reviewing the initial draft of the survey and helped with translating the survey\n－Sampling \\＆response rates：The sampling strategy was complex．［4］But in using stratified sampling for editors by wikimedia project．we had $\\sim 5,100$ responses，with response rates between $7.6 \\%$ to $53.3 \\%$ ．\n－Question design：An initial mapping of movement measures across various audiences was uncovered through this work.\n\nWhat needs improvement？\n\nWhile a more robust retrospective is pending for next quarter，a few initial improvements have been identified：\n－Too many questions\nIn this first year，the criteria for selecting questions was very loose in that the questions should align to the team＇s overall goals．What could be done to fix this problem？\n－Improving the translation process： Volunteer translation took a lot of time for staff and volunteers．There was likely not enough time for volunteer translation of the survey． What could be done to fix this problem？\n－Improving community collaboration： Surveys are complex in the Wikimedia movement，where survey respondents are also often the ones who want to ask the question． How could we build a collaborative survey process？\n－Improvements to the sampling process： For very active editors，we had some very high response rates，but for active editors，our rates were much lower．Next year，we plan to work on increasing responses for all audiences．\n\nAcknowledgments\n\nThere are far too many people to thank，but here is a very short list：Benoit Evellin，User：Haytham Abulela，Maria Cruz，Montserrat Boix，Jorge Vargas Kalliope Tsouroupidou，Erica LItrenta，Jeff Elder， User：Wargo，Nick Quiddity，User：Tsu－ya－，Neil Quinn，Jaime Anstee，User：Lyzzy，Lena Traer， User：Giaccai，User：Lucas，User：Eduardogobi， User：Frigory，Sati Houston，Vira Motorko， User：Takot，Cornelius Kibelka，User：Ata，User：星耀晨㬢 Delphine Menard，Sherry Snyder．．．and so many more！\n\nAbout 260 Questions．．．\nwere developed related to the following goals．The goals below represent a subset，since we cannot fit them all：\n\nWikimedia Movement\nimproving community health\nimproving actual and/or perceived safety for all community members\nimproving understanding of fundraising needs for the movement\nimprove knowledge and capacity related to policy issues\nWikimedia Projects\nimprove new editor engagement\nincreasing capacity for GLAM and Libraries\ncreating proactive public messaging strategy for Wikimedia projects\nimproving research literacy of editors\nWikimedia Software\nimproving technology tools for contributing\nimproving the editor area of software development\ngrowing the technical community with new developers\nWikimedia Programs & Affiliates\nimproving awareness & support for GLAM and Libraries\nincreasing capacity around evaluation for affiliates program leaders\neasing access to resources & people for program leaders/affiliates\nImproving support to Wikimedia affiliate organizations\nWikimedia Affiliates\nimproving communications related to affiliates/ program communities\nincreasing capacity of Wikimedia affiliates\nImprove collaboration with affiliates related to partnerships\nWikimedia Foundation\nimproving collaboration and communications between Foundation and communities\nimproving various Foundation programs, such as Global Reach, fundraising, products, code of conduct, etc.\n\nReport forthcoming May 2017！［5］",
        "page_idx": 0
    }
]