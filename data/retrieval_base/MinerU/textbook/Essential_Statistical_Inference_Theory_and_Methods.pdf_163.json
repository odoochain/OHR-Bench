[
    {
        "page_idx": 0,
        "text": "Table3.3 $P$ -valuesforthree samples, $n_{1}=n_{2}=n_{3}=5.$ $\\sigma^{2}=14$ \n\\begin{tabular}{llllll}\nY1 & Y2 & Y3 & ANOVA & ISO & Reg \\\\\n2 & 6 & 6 & 0.149 & 0.050 & 0.045 \\\\\n2 & 6 & 7 & 0.082 & 0.026 & 0.017 \\\\\n2 & 6 & 8 & 0.036 & 0.011 & 0.006 \\\\\n\\end{tabular}\n\n\nrelative ease of using regression is probably why isotonic regression is not used more. \nFinally,although the $p$ -values reported in Table 3.3 are for the known-variance case, qualitatively similar results are obtained for the more complicated case of $\\sigma$ unknown. $\\spadesuit$ \nThereisalargeliterature on order-restrictedinference.For testingfor ordered alternatives, there has been more emphasis on $T_{\\mathrm{LR}}$ thanon $T_{\\mathrm{{W}}}$ and $T_{\\mathrm{{S}}}$ Theclassic references are Barlow et al. (1972) and Robertson et al. (1988), whereas a more recent account isSilvapulle and Sen (2005). \n# \nWhen anull hypothesisvalue, say $\\theta_{0}$ lies on the boundary of the parameter space, thenmaximumlikelihoodestimators areoften truncated atthatboundarybecause by definition $\\widehat{\\theta}_{\\mathrm{MLE}}$ must lie in the parameter space of $\\theta$ . Thus $\\widehat{\\theta}_{\\mathrm{MLE}}$ is equal to the boundary value $\\theta_{0}$ with positive probability and correspondingly $T_{\\mathrm{LR}}$ is zero for those cases.The result is that the limiting distribution of $T_{\\mathrm{LR}}$ is a mixture of a point massatzero and a chi-squared distribution.We illustratefirstwith an artificial example and then consider the one-way random effects model. \n# \nSuppose that $Y_{1},\\dots,Y_{n}\\sim N(\\mu,1)$ Usually, $\\widehat{\\mu}_{\\mathrm{MLE}}=\\overline{{Y}}$ ,but suppose that we restrict theparameter space for $\\mu$ tobe $[\\mu_{0},\\infty)$ where $\\mu_{0}$ is some given constant, instead of $(-\\infty,\\infty)$ .Then ${\\widehat{\\mu}}_{\\mathrm{MLE}}={\\overline{{Y}}}$ if $\\overline{{Y}}~\\geq~\\mu_{0}$ and $\\widehat{\\mu}_{\\mathrm{MLE}}=\\mu_{0}$ if $\\overline{{Y}}<\\mu_{0}$ Now suppose that the null hypothesis is $H_{0}:\\mu=\\mu_{0}$ Wefirst consider the three likelihood-based test statistics, showing that only the score statistic has a limiting $\\chi_{1}^{2}$ distribution.Then we provide a simple solution to this testing problem. \nUnder $H_{0}$ ,the Wald statistic is $T_{\\mathrm{W}}=n(\\widehat{\\mu}_{\\mathrm{MLE}}-\\mu_{0})^{2}$ ,which is thus $T_{\\mathrm{W}}=0$ if $\\widehat{\\mu}_{\\mathrm{MLE}}=\\mu_{0}$ and $T_{\\mathrm{{W}}}=n({\\overline{{Y}}}-\\mu_{0})^{2}$ if $\\overline{{Y}}\\geq\\mu_{0}$ The score statistic is $T_{\\mathrm{S}}=n(Y-\\mu_{0})^{2}$ and thelikelihoodratio statisticis the same as theWald statistic.Thus,only the score statistic converges to a $\\chi_{1}^{2}$ distribution under $H_{0}$ .TheWald and thelikelihood ratio statistics converge to a distribution that is an equal mixture of a point mass at Oand a $\\chi_{1}^{2}$ distribution,the same distribution as in(3.23) for $k=2$ Infactthe"
    }
]